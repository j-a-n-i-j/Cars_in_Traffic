{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29047,"sourceType":"datasetVersion","datasetId":22655}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\n# Define dataset paths\nbase_path = '/kaggle/input/cityscapes-image-pairs/cityscapes_data'  # Update this path based on your Kaggle dataset location\nouter_train_path = os.path.join(base_path, 'train')\nouter_val_path = os.path.join(base_path, 'val')\ninner_train_path = os.path.join(base_path, 'cityscapes_data', 'train')\ninner_val_path = os.path.join(base_path, 'cityscapes_data', 'val')\n\n# Function to load and preprocess images\ndef load_and_preprocess_image(image_path, target_size=(150, 150)):\n    img = cv2.imread(image_path)\n    img = cv2.resize(img, target_size)\n    img = img / 255.0  # Normalize to [0, 1]\n    return img\n\ndef load_images_from_folder(folder_path, target_size=(150, 150)):\n    images = []\n    for img_name in os.listdir(folder_path):\n        img_path = os.path.join(folder_path, img_name)\n        if os.path.isfile(img_path):\n            img = load_and_preprocess_image(img_path, target_size)\n            images.append(img)\n    return images\n\ndef load_dataset(base_path, target_size=(150, 150)):\n    # Load images from all relevant folders\n    train_images = load_images_from_folder(os.path.join(base_path, 'train'), target_size)\n    val_images = load_images_from_folder(os.path.join(base_path, 'val'), target_size)\n    inner_train_images = load_images_from_folder(os.path.join(base_path, 'cityscapes_data', 'train'), target_size)\n    inner_val_images = load_images_from_folder(os.path.join(base_path, 'cityscapes_data', 'val'), target_size)\n    \n    # Combine all images\n    all_images = train_images + val_images + inner_train_images + inner_val_images\n    return np.array(all_images)\n\n# Load the dataset\nimages = load_dataset(base_path)\n\n# Create dummy labels for illustration (update with actual labels)\nnum_classes = 4  # Assuming 4 car colors\nlabels = np.random.randint(0, 2, size=(len(images),))  # Dummy binary labels\n\n# Convert labels to categorical (one-hot encoding) for color prediction\ncolor_labels = to_categorical(np.random.randint(0, num_classes, size=(len(images),)), num_classes=num_classes)\n\n# Create dummy car counts, male counts, and female counts\ncar_count_labels = np.random.randint(0, 10, size=(len(images), 1))\nmale_count_labels = np.random.randint(0, 10, size=(len(images), 1))\nfemale_count_labels = np.random.randint(0, 10, size=(len(images), 1))\n\n# Train-test split\ntrain_images, test_images, train_color_labels, test_color_labels, train_car_count_labels, test_car_count_labels, train_male_count_labels, test_male_count_labels, train_female_count_labels, test_female_count_labels = train_test_split(\n    images, color_labels, car_count_labels, male_count_labels, female_count_labels, test_size=0.2, random_state=42)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-12T08:42:19.971068Z","iopub.execute_input":"2024-08-12T08:42:19.971492Z","iopub.status.idle":"2024-08-12T08:44:22.113896Z","shell.execute_reply.started":"2024-08-12T08:42:19.971458Z","shell.execute_reply":"2024-08-12T08:44:22.112616Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-12 08:42:24.261319: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-12 08:42:24.261572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-12 08:42:24.445079: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to create the model\ndef create_model():\n    input_layer = layers.Input(shape=(150, 150, 3))\n    x = layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n    x = layers.MaxPooling2D((2, 2))(x)\n    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(512, activation='relu')(x)\n    \n    # Output layers\n    output_color = layers.Dense(num_classes, activation='softmax', name='color_output')(x) # Assuming 4 car colors\n    output_car_count = layers.Dense(1, activation='linear', name='car_count_output')(x)\n    output_male_count = layers.Dense(1, activation='linear', name='male_count_output')(x)\n    output_female_count = layers.Dense(1, activation='linear', name='female_count_output')(x)\n\n    model = models.Model(inputs=input_layer, outputs=[output_color, output_car_count, output_male_count, output_female_count])\n    \n    model.compile(optimizer='adam', \n                  loss={'color_output': 'categorical_crossentropy', \n                        'car_count_output': 'mean_squared_error',\n                        'male_count_output': 'mean_squared_error',\n                        'female_count_output': 'mean_squared_error'},\n                  metrics={'color_output': 'accuracy', \n                           'car_count_output': 'mse',\n                           'male_count_output': 'mse',\n                           'female_count_output': 'mse'})\n    return model\n\nmodel = create_model()\n\n# Train the model\nhistory = model.fit(train_images, \n                    {'color_output': train_color_labels,\n                     'car_count_output': train_car_count_labels,\n                     'male_count_output': train_male_count_labels,\n                     'female_count_output': train_female_count_labels},\n                    epochs=10,\n                    validation_data=(test_images, \n                                     {'color_output': test_color_labels,\n                                      'car_count_output': test_car_count_labels,\n                                      'male_count_output': test_male_count_labels,\n                                      'female_count_output': test_female_count_labels}))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T08:44:55.716520Z","iopub.execute_input":"2024-08-12T08:44:55.717488Z","iopub.status.idle":"2024-08-12T09:13:21.467729Z","shell.execute_reply.started":"2024-08-12T08:44:55.717444Z","shell.execute_reply":"2024-08-12T09:13:21.465195Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 958ms/step - car_count_output_mse: 9.7630 - color_output_accuracy: 0.2513 - female_count_output_mse: 9.6529 - loss: 30.6371 - male_count_output_mse: 9.8206 - val_car_count_output_mse: 8.8686 - val_color_output_accuracy: 0.2604 - val_female_count_output_mse: 8.4343 - val_loss: 26.8950 - val_male_count_output_mse: 8.1969\nEpoch 2/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 862ms/step - car_count_output_mse: 8.6000 - color_output_accuracy: 0.2501 - female_count_output_mse: 8.6191 - loss: 27.1184 - male_count_output_mse: 8.5047 - val_car_count_output_mse: 8.3546 - val_color_output_accuracy: 0.2619 - val_female_count_output_mse: 8.5008 - val_loss: 26.4897 - val_male_count_output_mse: 8.2465\nEpoch 3/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 858ms/step - car_count_output_mse: 8.5390 - color_output_accuracy: 0.2602 - female_count_output_mse: 8.2382 - loss: 26.5557 - male_count_output_mse: 8.3824 - val_car_count_output_mse: 8.5095 - val_color_output_accuracy: 0.2583 - val_female_count_output_mse: 8.3763 - val_loss: 26.8099 - val_male_count_output_mse: 8.5282\nEpoch 4/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 923ms/step - car_count_output_mse: 8.5922 - color_output_accuracy: 0.2534 - female_count_output_mse: 8.1800 - loss: 26.5613 - male_count_output_mse: 8.3945 - val_car_count_output_mse: 8.4489 - val_color_output_accuracy: 0.2604 - val_female_count_output_mse: 8.3421 - val_loss: 26.4914 - val_male_count_output_mse: 8.3017\nEpoch 5/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 958ms/step - car_count_output_mse: 8.4404 - color_output_accuracy: 0.2446 - female_count_output_mse: 8.2231 - loss: 26.3649 - male_count_output_mse: 8.3030 - val_car_count_output_mse: 8.3552 - val_color_output_accuracy: 0.2604 - val_female_count_output_mse: 8.3662 - val_loss: 26.6419 - val_male_count_output_mse: 8.5288\nEpoch 6/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 861ms/step - car_count_output_mse: 8.5572 - color_output_accuracy: 0.2435 - female_count_output_mse: 8.2023 - loss: 26.6456 - male_count_output_mse: 8.4929 - val_car_count_output_mse: 8.5431 - val_color_output_accuracy: 0.2432 - val_female_count_output_mse: 8.3913 - val_loss: 26.4257 - val_male_count_output_mse: 8.1000\nEpoch 7/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 859ms/step - car_count_output_mse: 8.5109 - color_output_accuracy: 0.2523 - female_count_output_mse: 8.1625 - loss: 26.4467 - male_count_output_mse: 8.3822 - val_car_count_output_mse: 8.5799 - val_color_output_accuracy: 0.2604 - val_female_count_output_mse: 8.3521 - val_loss: 26.4279 - val_male_count_output_mse: 8.1056\nEpoch 8/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 860ms/step - car_count_output_mse: 8.3736 - color_output_accuracy: 0.2466 - female_count_output_mse: 8.1790 - loss: 26.1922 - male_count_output_mse: 8.2494 - val_car_count_output_mse: 8.4213 - val_color_output_accuracy: 0.2604 - val_female_count_output_mse: 8.6796 - val_loss: 26.9570 - val_male_count_output_mse: 8.4669\nEpoch 9/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 932ms/step - car_count_output_mse: 8.1658 - color_output_accuracy: 0.2553 - female_count_output_mse: 8.1998 - loss: 26.1522 - male_count_output_mse: 8.3945 - val_car_count_output_mse: 8.4148 - val_color_output_accuracy: 0.2345 - val_female_count_output_mse: 8.3604 - val_loss: 26.2921 - val_male_count_output_mse: 8.1190\nEpoch 10/10\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 864ms/step - car_count_output_mse: 8.4403 - color_output_accuracy: 0.2574 - female_count_output_mse: 8.3008 - loss: 26.5520 - male_count_output_mse: 8.4139 - val_car_count_output_mse: 8.6749 - val_color_output_accuracy: 0.2432 - val_female_count_output_mse: 8.3420 - val_loss: 26.5536 - val_male_count_output_mse: 8.1297\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test set\nevaluation = model.evaluate(test_images, \n                            {'color_output': test_color_labels,\n                             'car_count_output': test_car_count_labels,\n                             'male_count_output': test_male_count_labels,\n                             'female_count_output': test_female_count_labels})\nprint(f\"Test Loss: {evaluation[0]}\")\nprint(f\"Test Accuracy (Color Prediction): {evaluation[1]}\")\n\n# Save the final model as a .keras file (new standard format)\nmodel.save('final_model.keras')\n\n# Save the final model as an .h5 file (legacy format)\nmodel.save('final_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:16:59.760899Z","iopub.execute_input":"2024-08-12T09:16:59.761503Z","iopub.status.idle":"2024-08-12T09:17:12.063328Z","shell.execute_reply.started":"2024-08-12T09:16:59.761450Z","shell.execute_reply":"2024-08-12T09:17:12.062070Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 239ms/step - car_count_output_mse: 9.0815 - color_output_accuracy: 0.2458 - female_count_output_mse: 8.5570 - loss: 27.0689 - male_count_output_mse: 8.0225\nTest Loss: 26.553619384765625\nTest Accuracy (Color Prediction): 8.674946784973145\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}